{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2Exp1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamaKrisnakommineni/INFO7374/blob/master/Assignment2/Part%202/A2Exp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "77r4X7-ozdMG",
        "colab_type": "code",
        "outputId": "b0d8e8fe-2370-4c50-d038-41fdb3d81c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import os, zipfile, io, requests\n",
        "URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "def download_images(url):\n",
        "    r = requests.get(url, stream=True)\n",
        "    print ('Downloading ' + url )\n",
        "    zip_ref = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    zip_ref.extractall('./')\n",
        "    zip_ref.close()\n",
        "download_images(URL) #To download"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://cs231n.stanford.edu/tiny-imagenet-200.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5n63NaDizhV9",
        "colab_type": "code",
        "outputId": "9f379ff1-25f3-480e-e984-def57fd16b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import six.moves.cPickle as pickle\n",
        "\n",
        "data = {}\n",
        "data['train'] = {}\n",
        "data['test'] = {}\n",
        "data['train']['data'] = []\n",
        "data['train']['target'] = []\n",
        "data['test']['data'] = []\n",
        "data['test']['target'] = []\n",
        "size = (32, 32)\n",
        "N = 400 ##400 as testing and 100 as training in each class\n",
        "\n",
        "wnids = list(map(lambda x: x.strip(), open('tiny-imagenet-200/wnids.txt').readlines()))\n",
        "\n",
        "for i in range(len(wnids)):\n",
        "    wnid = wnids[i]\n",
        "    print (\"{}: {} / {}\".format(wnid, i + 1, len(wnids)))\n",
        "    for j in range(500):\n",
        "        path = \"tiny-imagenet-200/train/{0}/images/{0}_{1}.JPEG\".format(wnid, j)\n",
        "        image = (Image.open(path).convert('RGB'))\n",
        "        image = image.resize(size, Image.ANTIALIAS)\n",
        "        image = np.array(image)\n",
        "        if j < N:\n",
        "            data['train']['data'].append(image)\n",
        "            data['train']['target'].append(i)\n",
        "        else:\n",
        "            data['test']['data'].append(image)\n",
        "            data['test']['target'].append(i)\n",
        "            \n",
        "\n",
        "print (\"Dump to train.pkl...\")\n",
        "pickle.dump(data, open('train.pkl', 'wb', -1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n07715103: 199 / 200\n",
            "n02504458: 200 / 200\n",
            "Dump to train.pkl...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwR4yYm_1RIW",
        "colab_type": "code",
        "outputId": "1f6918bd-bc12-4c84-b0cd-e4ed62b42121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import six.moves.cPickle as pickle\n",
        "data=pickle.load(open(\"train.pkl\",'rb'))\n",
        "#data['train']['data'][750]\n",
        "plt.imshow(data['train']['data'][50])\n",
        "plt.show()\n",
        "print(data['train']['target'][950])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZElEpMBkSUSk8P8Be33lgB7SGIYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kPGGn8uoIlzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Experiment 1"
      ]
    },
    {
      "metadata": {
        "id": "8WNXkfLLIq2B",
        "colab_type": "code",
        "outputId": "d27fa0f8-fe61-4d03-851d-6b40bb87e05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "from time import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = (data['train']['data'],data['train']['target']),(data['test']['data'],data['test']['target'])\n",
        "\n",
        "num_classes=200\n",
        "\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "x_train =x_train/ 255\n",
        "x_test = x_test/255\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CqbstOh4W-lx",
        "colab_type": "code",
        "outputId": "7075a8d3-d7e8-41f5-b8fb-eab528868960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9291
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "opts = [('SGD', SGD()), ('Adagrad', Adagrad()), ('Adadelta', Adadelta()), \n",
        "        ('Adam', Adam()), ('Adamax', Adamax())]\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "Accuracy_list = []\n",
        "for name,opt in opts:\n",
        "    print('Training ' + name + ' optimizer')\n",
        "    logs = \"logs/optimizer/\"+name\n",
        "    \n",
        "    model_name = name + 'Optmizer for TinyImagenet200'\n",
        "\n",
        "  # Let's train the model using RMSprop\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "    start = time()\n",
        "  \n",
        "    if not data_augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        model.fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  shuffle=True)\n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            steps_per_epoch=500)\n",
        "\n",
        "    # Save model and weights\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    model_path = os.path.join(save_dir, model_name)\n",
        "    model.save(model_path)\n",
        "    print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "    end = time()\n",
        "\n",
        "    print('Test loss:', scores[0])\n",
        "    print('Test accuracy:', scores[1])\n",
        "    print('Time taken to compile:', str(end-start))\n",
        "    Accuracy_list.append([name,scores[1]])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training SGD optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 5.2983 - acc: 0.0048 - val_loss: 5.2972 - val_acc: 0.0049\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2975 - acc: 0.0057 - val_loss: 5.2963 - val_acc: 0.0062\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2958 - acc: 0.0062 - val_loss: 5.2931 - val_acc: 0.0092\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2906 - acc: 0.0073 - val_loss: 5.2819 - val_acc: 0.0092\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2765 - acc: 0.0090 - val_loss: 5.2449 - val_acc: 0.0097\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2344 - acc: 0.0112 - val_loss: 5.1902 - val_acc: 0.0100\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.2066 - acc: 0.0094 - val_loss: 5.1600 - val_acc: 0.0109\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.1756 - acc: 0.0104 - val_loss: 5.1349 - val_acc: 0.0129\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.1684 - acc: 0.0111 - val_loss: 5.1169 - val_acc: 0.0163\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.1530 - acc: 0.0125 - val_loss: 5.1199 - val_acc: 0.0145\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.1391 - acc: 0.0138 - val_loss: 5.0969 - val_acc: 0.0162\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.1310 - acc: 0.0134 - val_loss: 5.0788 - val_acc: 0.0178\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.1140 - acc: 0.0159 - val_loss: 5.0675 - val_acc: 0.0198\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.1160 - acc: 0.0177 - val_loss: 5.0485 - val_acc: 0.0225\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.1006 - acc: 0.0169 - val_loss: 5.0353 - val_acc: 0.0261\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.0838 - acc: 0.0187 - val_loss: 5.0205 - val_acc: 0.0272\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 5.0688 - acc: 0.0194 - val_loss: 5.0012 - val_acc: 0.0338\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.0487 - acc: 0.0226 - val_loss: 4.9631 - val_acc: 0.0370\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 5.0130 - acc: 0.0282 - val_loss: 4.9490 - val_acc: 0.0376\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.9969 - acc: 0.0284 - val_loss: 4.9135 - val_acc: 0.0439\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.9764 - acc: 0.0304 - val_loss: 4.8886 - val_acc: 0.0432\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.9562 - acc: 0.0316 - val_loss: 4.8607 - val_acc: 0.0460\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.9402 - acc: 0.0316 - val_loss: 4.8647 - val_acc: 0.0476\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.9252 - acc: 0.0331 - val_loss: 4.8030 - val_acc: 0.0512\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.9036 - acc: 0.0362 - val_loss: 4.8217 - val_acc: 0.0512\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.8744 - acc: 0.0402 - val_loss: 4.7696 - val_acc: 0.0559\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.8727 - acc: 0.0384 - val_loss: 4.7441 - val_acc: 0.0580\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.8367 - acc: 0.0416 - val_loss: 4.7333 - val_acc: 0.0589\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.8322 - acc: 0.0426 - val_loss: 4.6966 - val_acc: 0.0605\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.8276 - acc: 0.0425 - val_loss: 4.7255 - val_acc: 0.0593\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.7940 - acc: 0.0474 - val_loss: 4.6718 - val_acc: 0.0654\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.7782 - acc: 0.0472 - val_loss: 4.6449 - val_acc: 0.0677\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.7685 - acc: 0.0500 - val_loss: 4.6709 - val_acc: 0.0665\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.7490 - acc: 0.0531 - val_loss: 4.6113 - val_acc: 0.0708\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.7295 - acc: 0.0508 - val_loss: 4.5976 - val_acc: 0.0733\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.6979 - acc: 0.0523 - val_loss: 4.5948 - val_acc: 0.0664\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.6954 - acc: 0.0584 - val_loss: 4.5545 - val_acc: 0.0763\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6867 - acc: 0.0598 - val_loss: 4.5508 - val_acc: 0.0781\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6680 - acc: 0.0588 - val_loss: 4.5272 - val_acc: 0.0794\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6622 - acc: 0.0656 - val_loss: 4.5156 - val_acc: 0.0810\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6324 - acc: 0.0630 - val_loss: 4.5023 - val_acc: 0.0837\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.6166 - acc: 0.0668 - val_loss: 4.4760 - val_acc: 0.0846\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6211 - acc: 0.0667 - val_loss: 4.4929 - val_acc: 0.0832\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.6208 - acc: 0.0694 - val_loss: 4.4597 - val_acc: 0.0891\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5827 - acc: 0.0719 - val_loss: 4.4350 - val_acc: 0.0935\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5555 - acc: 0.0740 - val_loss: 4.4231 - val_acc: 0.0925\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5487 - acc: 0.0751 - val_loss: 4.4153 - val_acc: 0.0936\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 4.5599 - acc: 0.0734 - val_loss: 4.3717 - val_acc: 0.1019\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5276 - acc: 0.0749 - val_loss: 4.3658 - val_acc: 0.1037\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5271 - acc: 0.0792 - val_loss: 4.3411 - val_acc: 0.1036\n",
            "Saved trained model at /content/saved_models/SGDOptmizer for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 171us/step\n",
            "Test loss: 4.3411163845062255\n",
            "Test accuracy: 0.1036\n",
            "Time taken to compile: 742.7864799499512\n",
            "Training Adagrad optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.7865 - acc: 0.0508 - val_loss: 4.4970 - val_acc: 0.0790\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.5058 - acc: 0.0789 - val_loss: 4.3147 - val_acc: 0.1038\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.4556 - acc: 0.0821 - val_loss: 4.2358 - val_acc: 0.1141\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.3648 - acc: 0.0965 - val_loss: 4.1644 - val_acc: 0.1247\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.3234 - acc: 0.1023 - val_loss: 4.1182 - val_acc: 0.1316\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.2483 - acc: 0.1098 - val_loss: 4.0873 - val_acc: 0.1346\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 4.2242 - acc: 0.1154 - val_loss: 4.0328 - val_acc: 0.1422\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 4.1830 - acc: 0.1176 - val_loss: 4.0546 - val_acc: 0.1396\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.1826 - acc: 0.1191 - val_loss: 3.9999 - val_acc: 0.1482\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.1750 - acc: 0.1198 - val_loss: 3.9877 - val_acc: 0.1514\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.1050 - acc: 0.1315 - val_loss: 3.9282 - val_acc: 0.1613\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 4.0916 - acc: 0.1316 - val_loss: 3.9184 - val_acc: 0.1623\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0748 - acc: 0.1364 - val_loss: 3.9474 - val_acc: 0.1555\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0757 - acc: 0.1323 - val_loss: 3.8854 - val_acc: 0.1653\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0624 - acc: 0.1414 - val_loss: 3.8765 - val_acc: 0.1664\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0171 - acc: 0.1389 - val_loss: 3.8589 - val_acc: 0.1709\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0151 - acc: 0.1451 - val_loss: 3.8585 - val_acc: 0.1690\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.9838 - acc: 0.1512 - val_loss: 3.8243 - val_acc: 0.1754\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.9988 - acc: 0.1453 - val_loss: 3.8057 - val_acc: 0.1771\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.9749 - acc: 0.1523 - val_loss: 3.8179 - val_acc: 0.1736\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.9303 - acc: 0.1574 - val_loss: 3.7923 - val_acc: 0.1779\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9333 - acc: 0.1551 - val_loss: 3.8016 - val_acc: 0.1787\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.9116 - acc: 0.1602 - val_loss: 3.7628 - val_acc: 0.1848\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9440 - acc: 0.1526 - val_loss: 3.7712 - val_acc: 0.1828\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.9319 - acc: 0.1525 - val_loss: 3.7204 - val_acc: 0.1911\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8920 - acc: 0.1637 - val_loss: 3.7414 - val_acc: 0.1898\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8820 - acc: 0.1623 - val_loss: 3.7383 - val_acc: 0.1903\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.8828 - acc: 0.1686 - val_loss: 3.7031 - val_acc: 0.1953\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8765 - acc: 0.1641 - val_loss: 3.7105 - val_acc: 0.1928\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8683 - acc: 0.1620 - val_loss: 3.6766 - val_acc: 0.1988\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.8408 - acc: 0.1734 - val_loss: 3.7105 - val_acc: 0.1942\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8561 - acc: 0.1653 - val_loss: 3.6692 - val_acc: 0.1998\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 14s 29ms/step - loss: 3.8321 - acc: 0.1719 - val_loss: 3.6915 - val_acc: 0.1966\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8169 - acc: 0.1736 - val_loss: 3.6594 - val_acc: 0.2026\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8219 - acc: 0.1753 - val_loss: 3.6907 - val_acc: 0.1989\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8114 - acc: 0.1727 - val_loss: 3.6538 - val_acc: 0.2032\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7769 - acc: 0.1757 - val_loss: 3.6319 - val_acc: 0.2069\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.8067 - acc: 0.1727 - val_loss: 3.6365 - val_acc: 0.2050\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7680 - acc: 0.1825 - val_loss: 3.6337 - val_acc: 0.2061\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7999 - acc: 0.1749 - val_loss: 3.6555 - val_acc: 0.2027\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7810 - acc: 0.1791 - val_loss: 3.6015 - val_acc: 0.2127\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.7660 - acc: 0.1813 - val_loss: 3.6086 - val_acc: 0.2091\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7500 - acc: 0.1836 - val_loss: 3.6287 - val_acc: 0.2054\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7360 - acc: 0.1835 - val_loss: 3.6166 - val_acc: 0.2074\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7617 - acc: 0.1835 - val_loss: 3.6008 - val_acc: 0.2123\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7278 - acc: 0.1869 - val_loss: 3.6167 - val_acc: 0.2084\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7315 - acc: 0.1843 - val_loss: 3.6148 - val_acc: 0.2097\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.7487 - acc: 0.1787 - val_loss: 3.5728 - val_acc: 0.2180\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7187 - acc: 0.1891 - val_loss: 3.5738 - val_acc: 0.2165\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7096 - acc: 0.1911 - val_loss: 3.5921 - val_acc: 0.2138\n",
            "Saved trained model at /content/saved_models/AdagradOptmizer for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 171us/step\n",
            "Test loss: 3.5921390083312987\n",
            "Test accuracy: 0.2138\n",
            "Time taken to compile: 739.4897727966309\n",
            "Training Adadelta optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0268 - acc: 0.1419 - val_loss: 3.8967 - val_acc: 0.1677\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 4.0081 - acc: 0.1469 - val_loss: 3.8033 - val_acc: 0.1750\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.0087 - acc: 0.1495 - val_loss: 3.7802 - val_acc: 0.1807\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.9814 - acc: 0.1559 - val_loss: 3.8269 - val_acc: 0.1739\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.9887 - acc: 0.1486 - val_loss: 3.7867 - val_acc: 0.1815\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.9196 - acc: 0.1568 - val_loss: 3.8704 - val_acc: 0.1694\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.9095 - acc: 0.1630 - val_loss: 3.7946 - val_acc: 0.1774\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.9288 - acc: 0.1609 - val_loss: 3.7455 - val_acc: 0.1883\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9049 - acc: 0.1640 - val_loss: 3.7074 - val_acc: 0.1944\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.9141 - acc: 0.1626 - val_loss: 3.7575 - val_acc: 0.1825\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8466 - acc: 0.1676 - val_loss: 3.6159 - val_acc: 0.2046\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8563 - acc: 0.1739 - val_loss: 3.6980 - val_acc: 0.1968\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.8576 - acc: 0.1714 - val_loss: 3.6118 - val_acc: 0.2099\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8563 - acc: 0.1706 - val_loss: 3.6375 - val_acc: 0.2036\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8566 - acc: 0.1704 - val_loss: 3.6467 - val_acc: 0.2018\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.8108 - acc: 0.1749 - val_loss: 3.5833 - val_acc: 0.2139\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.8180 - acc: 0.1764 - val_loss: 3.5802 - val_acc: 0.2156\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8124 - acc: 0.1756 - val_loss: 3.6913 - val_acc: 0.1957\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8237 - acc: 0.1751 - val_loss: 3.7300 - val_acc: 0.1893\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8514 - acc: 0.1739 - val_loss: 3.5975 - val_acc: 0.2077\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7880 - acc: 0.1821 - val_loss: 3.5952 - val_acc: 0.2084\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7934 - acc: 0.1831 - val_loss: 3.5588 - val_acc: 0.2154\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8018 - acc: 0.1820 - val_loss: 3.5772 - val_acc: 0.2140\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8080 - acc: 0.1778 - val_loss: 3.5395 - val_acc: 0.2196\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8051 - acc: 0.1824 - val_loss: 3.6635 - val_acc: 0.1967\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7425 - acc: 0.1889 - val_loss: 3.6124 - val_acc: 0.2094\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7565 - acc: 0.1856 - val_loss: 3.5286 - val_acc: 0.2200\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7695 - acc: 0.1895 - val_loss: 3.6112 - val_acc: 0.2079\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7920 - acc: 0.1863 - val_loss: 3.5980 - val_acc: 0.2075\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8029 - acc: 0.1788 - val_loss: 3.5716 - val_acc: 0.2132\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7466 - acc: 0.1920 - val_loss: 3.5034 - val_acc: 0.2271\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7576 - acc: 0.1866 - val_loss: 3.5756 - val_acc: 0.2147\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7765 - acc: 0.1839 - val_loss: 3.5307 - val_acc: 0.2200\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7677 - acc: 0.1867 - val_loss: 3.5811 - val_acc: 0.2162\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7928 - acc: 0.1801 - val_loss: 3.5812 - val_acc: 0.2111\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7384 - acc: 0.1903 - val_loss: 3.7822 - val_acc: 0.1849\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7560 - acc: 0.1881 - val_loss: 3.5985 - val_acc: 0.2106\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7747 - acc: 0.1847 - val_loss: 3.6265 - val_acc: 0.2047\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7563 - acc: 0.1892 - val_loss: 3.5335 - val_acc: 0.2191\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7858 - acc: 0.1866 - val_loss: 3.6243 - val_acc: 0.2054\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7333 - acc: 0.1868 - val_loss: 3.5596 - val_acc: 0.2169\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7272 - acc: 0.1893 - val_loss: 3.5860 - val_acc: 0.2098\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7403 - acc: 0.1942 - val_loss: 3.6209 - val_acc: 0.2059\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7896 - acc: 0.1812 - val_loss: 3.4773 - val_acc: 0.2303\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7738 - acc: 0.1885 - val_loss: 3.5964 - val_acc: 0.2129\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7397 - acc: 0.1942 - val_loss: 3.6196 - val_acc: 0.2071\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7645 - acc: 0.1869 - val_loss: 3.6484 - val_acc: 0.2043\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7578 - acc: 0.1934 - val_loss: 3.5359 - val_acc: 0.2219\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7647 - acc: 0.1853 - val_loss: 3.6696 - val_acc: 0.1964\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.7729 - acc: 0.1857 - val_loss: 3.5151 - val_acc: 0.2238\n",
            "Saved trained model at /content/saved_models/AdadeltaOptmizer for TinyImagenet200 \n",
            "20000/20000 [==============================] - 4s 175us/step\n",
            "Test loss: 3.5151386296272276\n",
            "Test accuracy: 0.22385\n",
            "Time taken to compile: 782.5443997383118\n",
            "Training Adam optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7677 - acc: 0.1871 - val_loss: 3.5173 - val_acc: 0.2192\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7760 - acc: 0.1839 - val_loss: 3.6797 - val_acc: 0.1923\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7679 - acc: 0.1860 - val_loss: 3.5184 - val_acc: 0.2242\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7501 - acc: 0.1877 - val_loss: 3.6746 - val_acc: 0.1991\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7572 - acc: 0.1933 - val_loss: 3.5894 - val_acc: 0.2121\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7065 - acc: 0.1946 - val_loss: 3.5260 - val_acc: 0.2226\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6962 - acc: 0.1979 - val_loss: 3.5134 - val_acc: 0.2238\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7049 - acc: 0.1919 - val_loss: 3.5051 - val_acc: 0.2227\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7220 - acc: 0.1908 - val_loss: 3.5119 - val_acc: 0.2213\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7051 - acc: 0.1924 - val_loss: 3.5048 - val_acc: 0.2215\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.6779 - acc: 0.1983 - val_loss: 3.5511 - val_acc: 0.2160\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6569 - acc: 0.2001 - val_loss: 3.4540 - val_acc: 0.2319\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.6942 - acc: 0.1948 - val_loss: 3.5425 - val_acc: 0.2140\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6961 - acc: 0.1930 - val_loss: 3.4482 - val_acc: 0.2298\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6858 - acc: 0.1916 - val_loss: 3.5153 - val_acc: 0.2212\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6479 - acc: 0.2009 - val_loss: 3.6450 - val_acc: 0.2032\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6525 - acc: 0.1997 - val_loss: 3.4475 - val_acc: 0.2336\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6512 - acc: 0.2008 - val_loss: 3.4916 - val_acc: 0.2279\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6780 - acc: 0.1959 - val_loss: 3.4549 - val_acc: 0.2324\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6678 - acc: 0.1950 - val_loss: 3.4865 - val_acc: 0.2255\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6324 - acc: 0.1985 - val_loss: 3.4738 - val_acc: 0.2286\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6437 - acc: 0.1986 - val_loss: 3.4458 - val_acc: 0.2354\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6652 - acc: 0.1995 - val_loss: 3.4457 - val_acc: 0.2384\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6964 - acc: 0.1913 - val_loss: 3.4675 - val_acc: 0.2343\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6447 - acc: 0.1994 - val_loss: 3.4867 - val_acc: 0.2306\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6208 - acc: 0.2047 - val_loss: 3.5205 - val_acc: 0.2231\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.6485 - acc: 0.1983 - val_loss: 3.4587 - val_acc: 0.2335\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6673 - acc: 0.1967 - val_loss: 3.5388 - val_acc: 0.2193\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6293 - acc: 0.2005 - val_loss: 3.5162 - val_acc: 0.2268\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6317 - acc: 0.2010 - val_loss: 3.4682 - val_acc: 0.2303\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5695 - acc: 0.2103 - val_loss: 3.5111 - val_acc: 0.2223\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6059 - acc: 0.2048 - val_loss: 3.4858 - val_acc: 0.2268\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6051 - acc: 0.2124 - val_loss: 3.5101 - val_acc: 0.2236\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6439 - acc: 0.2024 - val_loss: 3.4932 - val_acc: 0.2265\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6465 - acc: 0.2009 - val_loss: 3.4634 - val_acc: 0.2293\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5467 - acc: 0.2071 - val_loss: 3.4341 - val_acc: 0.2409\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6034 - acc: 0.2048 - val_loss: 3.4235 - val_acc: 0.2400\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6145 - acc: 0.2032 - val_loss: 3.4609 - val_acc: 0.2311\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6174 - acc: 0.1998 - val_loss: 3.5238 - val_acc: 0.2188\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6280 - acc: 0.2018 - val_loss: 3.4349 - val_acc: 0.2346\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5623 - acc: 0.2151 - val_loss: 3.4324 - val_acc: 0.2339\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6117 - acc: 0.2020 - val_loss: 3.4444 - val_acc: 0.2336\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6182 - acc: 0.1988 - val_loss: 3.4400 - val_acc: 0.2355\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6183 - acc: 0.2023 - val_loss: 3.4078 - val_acc: 0.2412\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.6140 - acc: 0.2015 - val_loss: 3.5782 - val_acc: 0.2187\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5641 - acc: 0.2099 - val_loss: 3.4289 - val_acc: 0.2427\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5554 - acc: 0.2136 - val_loss: 3.3674 - val_acc: 0.2475\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6098 - acc: 0.2093 - val_loss: 3.5031 - val_acc: 0.2255\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.5831 - acc: 0.2070 - val_loss: 3.4700 - val_acc: 0.2339\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6308 - acc: 0.2027 - val_loss: 3.4342 - val_acc: 0.2360\n",
            "Saved trained model at /content/saved_models/AdamOptmizer for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 154us/step\n",
            "Test loss: 3.4342220067977904\n",
            "Test accuracy: 0.236\n",
            "Time taken to compile: 766.0635049343109\n",
            "Training Adamax optimizer\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5326 - acc: 0.2161 - val_loss: 3.4904 - val_acc: 0.2324\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4648 - acc: 0.2302 - val_loss: 3.4023 - val_acc: 0.2425\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4880 - acc: 0.2271 - val_loss: 3.3490 - val_acc: 0.2541\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4833 - acc: 0.2209 - val_loss: 3.3647 - val_acc: 0.2512\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4448 - acc: 0.2316 - val_loss: 3.3576 - val_acc: 0.2513\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4170 - acc: 0.2351 - val_loss: 3.3951 - val_acc: 0.2438\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4328 - acc: 0.2380 - val_loss: 3.3618 - val_acc: 0.2516\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4370 - acc: 0.2373 - val_loss: 3.3132 - val_acc: 0.2573\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4568 - acc: 0.2290 - val_loss: 3.4168 - val_acc: 0.2479\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4529 - acc: 0.2319 - val_loss: 3.3695 - val_acc: 0.2500\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4166 - acc: 0.2373 - val_loss: 3.3627 - val_acc: 0.2501\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3978 - acc: 0.2419 - val_loss: 3.3593 - val_acc: 0.2496\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3928 - acc: 0.2383 - val_loss: 3.3411 - val_acc: 0.2520\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4050 - acc: 0.2378 - val_loss: 3.3819 - val_acc: 0.2456\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4315 - acc: 0.2299 - val_loss: 3.2968 - val_acc: 0.2626\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3706 - acc: 0.2464 - val_loss: 3.3248 - val_acc: 0.2532\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3965 - acc: 0.2381 - val_loss: 3.2918 - val_acc: 0.2625\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4126 - acc: 0.2364 - val_loss: 3.3567 - val_acc: 0.2549\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3862 - acc: 0.2431 - val_loss: 3.3020 - val_acc: 0.2621\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.4248 - acc: 0.2343 - val_loss: 3.3008 - val_acc: 0.2601\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3928 - acc: 0.2383 - val_loss: 3.2853 - val_acc: 0.2636\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4003 - acc: 0.2436 - val_loss: 3.3390 - val_acc: 0.2559\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 3.3836 - acc: 0.2370 - val_loss: 3.3710 - val_acc: 0.2504\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3973 - acc: 0.2378 - val_loss: 3.2905 - val_acc: 0.2619\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3747 - acc: 0.2415 - val_loss: 3.3030 - val_acc: 0.2590\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3768 - acc: 0.2397 - val_loss: 3.2652 - val_acc: 0.2688\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3798 - acc: 0.2365 - val_loss: 3.3933 - val_acc: 0.2451\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3987 - acc: 0.2416 - val_loss: 3.2862 - val_acc: 0.2615\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3701 - acc: 0.2411 - val_loss: 3.3388 - val_acc: 0.2602\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3818 - acc: 0.2458 - val_loss: 3.2522 - val_acc: 0.2705\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3697 - acc: 0.2430 - val_loss: 3.3084 - val_acc: 0.2605\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3581 - acc: 0.2411 - val_loss: 3.2841 - val_acc: 0.2635\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3646 - acc: 0.2462 - val_loss: 3.2772 - val_acc: 0.2666\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3755 - acc: 0.2467 - val_loss: 3.4821 - val_acc: 0.2366\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3493 - acc: 0.2479 - val_loss: 3.3724 - val_acc: 0.2509\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3590 - acc: 0.2452 - val_loss: 3.2605 - val_acc: 0.2687\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3571 - acc: 0.2466 - val_loss: 3.2975 - val_acc: 0.2619\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3498 - acc: 0.2487 - val_loss: 3.2393 - val_acc: 0.2717\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3522 - acc: 0.2468 - val_loss: 3.3326 - val_acc: 0.2586\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3722 - acc: 0.2409 - val_loss: 3.3155 - val_acc: 0.2642\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3177 - acc: 0.2497 - val_loss: 3.2810 - val_acc: 0.2621\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3440 - acc: 0.2506 - val_loss: 3.2986 - val_acc: 0.2600\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3677 - acc: 0.2455 - val_loss: 3.1886 - val_acc: 0.2792\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3492 - acc: 0.2461 - val_loss: 3.2628 - val_acc: 0.2715\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3533 - acc: 0.2435 - val_loss: 3.2387 - val_acc: 0.2749\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3279 - acc: 0.2512 - val_loss: 3.3098 - val_acc: 0.2637\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3310 - acc: 0.2464 - val_loss: 3.3630 - val_acc: 0.2536\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3534 - acc: 0.2477 - val_loss: 3.2940 - val_acc: 0.2636\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3503 - acc: 0.2464 - val_loss: 3.2349 - val_acc: 0.2742\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3396 - acc: 0.2487 - val_loss: 3.2750 - val_acc: 0.2711\n",
            "Saved trained model at /content/saved_models/AdamaxOptmizer for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 157us/step\n",
            "Test loss: 3.2749884740829467\n",
            "Test accuracy: 0.2711\n",
            "Time taken to compile: 752.1706428527832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N86MZg21d7-y",
        "colab_type": "code",
        "outputId": "a726b94e-854c-4582-a85b-3e6d82fb0b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "Accuracy_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['SGD', 0.1036],\n",
              " ['Adagrad', 0.2138],\n",
              " ['Adadelta', 0.22385],\n",
              " ['Adam', 0.236],\n",
              " ['Adamax', 0.2711]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "oEYfXfBieGb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adamax optimizer got the maximum accuracy,\n",
        "**Now with Adamax optimizer we shall change activation functions**"
      ]
    },
    {
      "metadata": {
        "id": "Q5MkGWNleNDZ",
        "colab_type": "code",
        "outputId": "896dcbaf-0921-487e-c299-ad7c96769f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7765
        }
      },
      "cell_type": "code",
      "source": [
        "activations = ['tanh','relu','elu']\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "Activation_accuracy=[]\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "for acti in activations:\n",
        "  print('Training with' + acti + ' Activation function')\n",
        "  logs = \"logs/optimizer/\"+name\n",
        "    \n",
        "  model_name = acti + 'Activation for TinyImagenet200'\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(acti))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # initiate RMSprop optimizer\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adamax(),metrics=['accuracy'])\n",
        "\n",
        "  start = time()\n",
        "  \n",
        "  if not data_augmentation:\n",
        "      print('Not using data augmentation.')\n",
        "      model.fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  shuffle=True)\n",
        "  else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            steps_per_epoch=500)\n",
        "\n",
        "    # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        model_path = os.path.join(save_dir, model_name)\n",
        "        model.save(model_path)\n",
        "        print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "    # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "  end = time()\n",
        "\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  print('Time taken to compile:', str(end-start))\n",
        "  Activation_accuracy.append([acti,scores[1]])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training withtanh Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 5.1207 - acc: 0.0209 - val_loss: 4.9186 - val_acc: 0.0416\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.8788 - acc: 0.0456 - val_loss: 4.6749 - val_acc: 0.0629\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.7042 - acc: 0.0617 - val_loss: 4.4887 - val_acc: 0.0831\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.5720 - acc: 0.0768 - val_loss: 4.4378 - val_acc: 0.0930\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.4906 - acc: 0.0884 - val_loss: 4.3486 - val_acc: 0.1084\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3954 - acc: 0.0954 - val_loss: 4.3174 - val_acc: 0.1126\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3788 - acc: 0.1040 - val_loss: 4.2992 - val_acc: 0.1120\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3228 - acc: 0.1104 - val_loss: 4.2195 - val_acc: 0.1260\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3148 - acc: 0.1070 - val_loss: 4.2126 - val_acc: 0.1226\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3007 - acc: 0.1058 - val_loss: 4.2442 - val_acc: 0.1260\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2222 - acc: 0.1202 - val_loss: 4.2702 - val_acc: 0.1232\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2076 - acc: 0.1252 - val_loss: 4.3147 - val_acc: 0.1188\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2023 - acc: 0.1202 - val_loss: 4.1375 - val_acc: 0.1377\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1825 - acc: 0.1271 - val_loss: 4.3016 - val_acc: 0.1198\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1642 - acc: 0.1318 - val_loss: 4.1743 - val_acc: 0.1358\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1170 - acc: 0.1377 - val_loss: 4.0651 - val_acc: 0.1457\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1318 - acc: 0.1363 - val_loss: 4.0365 - val_acc: 0.1481\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1080 - acc: 0.1364 - val_loss: 4.1973 - val_acc: 0.1351\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1037 - acc: 0.1394 - val_loss: 4.1397 - val_acc: 0.1394\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0930 - acc: 0.1416 - val_loss: 4.0469 - val_acc: 0.1490\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0492 - acc: 0.1500 - val_loss: 3.9862 - val_acc: 0.1530\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0686 - acc: 0.1429 - val_loss: 4.0027 - val_acc: 0.1575\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0362 - acc: 0.1472 - val_loss: 3.8899 - val_acc: 0.1701\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0488 - acc: 0.1484 - val_loss: 3.9202 - val_acc: 0.1664\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0845 - acc: 0.1432 - val_loss: 4.0428 - val_acc: 0.1523\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9995 - acc: 0.1563 - val_loss: 4.1381 - val_acc: 0.1439\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0148 - acc: 0.1493 - val_loss: 3.9871 - val_acc: 0.1590\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0258 - acc: 0.1559 - val_loss: 4.0008 - val_acc: 0.1598\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0073 - acc: 0.1501 - val_loss: 3.8787 - val_acc: 0.1706\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0153 - acc: 0.1525 - val_loss: 3.9079 - val_acc: 0.1681\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9623 - acc: 0.1607 - val_loss: 4.1215 - val_acc: 0.1474\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9553 - acc: 0.1613 - val_loss: 3.8844 - val_acc: 0.1734\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9875 - acc: 0.1539 - val_loss: 3.8177 - val_acc: 0.1799\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0060 - acc: 0.1572 - val_loss: 3.8673 - val_acc: 0.1758\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9787 - acc: 0.1594 - val_loss: 3.9791 - val_acc: 0.1598\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9424 - acc: 0.1606 - val_loss: 3.9688 - val_acc: 0.1565\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9477 - acc: 0.1629 - val_loss: 4.0546 - val_acc: 0.1499\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9664 - acc: 0.1608 - val_loss: 3.8718 - val_acc: 0.1733\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9594 - acc: 0.1597 - val_loss: 4.0794 - val_acc: 0.1493\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9423 - acc: 0.1608 - val_loss: 3.8314 - val_acc: 0.1789\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9167 - acc: 0.1684 - val_loss: 3.8652 - val_acc: 0.1736\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9327 - acc: 0.1637 - val_loss: 3.9200 - val_acc: 0.1691\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9190 - acc: 0.1611 - val_loss: 3.9139 - val_acc: 0.1688\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9460 - acc: 0.1649 - val_loss: 3.8492 - val_acc: 0.1777\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9226 - acc: 0.1645 - val_loss: 3.8798 - val_acc: 0.1731\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8893 - acc: 0.1680 - val_loss: 3.8980 - val_acc: 0.1676\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9164 - acc: 0.1658 - val_loss: 3.9181 - val_acc: 0.1704\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9107 - acc: 0.1651 - val_loss: 3.9367 - val_acc: 0.1707\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9336 - acc: 0.1627 - val_loss: 3.8167 - val_acc: 0.1809\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9242 - acc: 0.1646 - val_loss: 3.8100 - val_acc: 0.1809\n",
            "20000/20000 [==============================] - 3s 163us/step\n",
            "Test loss: 3.81002687664032\n",
            "Test accuracy: 0.1809\n",
            "Time taken to compile: 760.3138151168823\n",
            "Training withrelu Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.2575 - acc: 0.0064 - val_loss: 5.1574 - val_acc: 0.0100\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 5.1541 - acc: 0.0116 - val_loss: 5.0747 - val_acc: 0.0193\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 5.0752 - acc: 0.0183 - val_loss: 4.9802 - val_acc: 0.0327\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.9806 - acc: 0.0258 - val_loss: 4.8799 - val_acc: 0.0411\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.8911 - acc: 0.0360 - val_loss: 4.7215 - val_acc: 0.0550\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.7793 - acc: 0.0460 - val_loss: 4.6137 - val_acc: 0.0679\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.7071 - acc: 0.0545 - val_loss: 4.5146 - val_acc: 0.0768\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.6199 - acc: 0.0641 - val_loss: 4.3869 - val_acc: 0.0929\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.5707 - acc: 0.0678 - val_loss: 4.3820 - val_acc: 0.0932\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.4922 - acc: 0.0747 - val_loss: 4.3047 - val_acc: 0.1006\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.4158 - acc: 0.0884 - val_loss: 4.2731 - val_acc: 0.1045\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3700 - acc: 0.0953 - val_loss: 4.2374 - val_acc: 0.1124\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3592 - acc: 0.0951 - val_loss: 4.1436 - val_acc: 0.1259\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.3088 - acc: 0.0989 - val_loss: 4.0775 - val_acc: 0.1356\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2844 - acc: 0.1054 - val_loss: 4.0333 - val_acc: 0.1429\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2145 - acc: 0.1109 - val_loss: 3.9923 - val_acc: 0.1464\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.1978 - acc: 0.1155 - val_loss: 4.0294 - val_acc: 0.1398\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1344 - acc: 0.1281 - val_loss: 3.9368 - val_acc: 0.1536\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.1335 - acc: 0.1275 - val_loss: 3.8824 - val_acc: 0.1588\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1189 - acc: 0.1274 - val_loss: 3.8694 - val_acc: 0.1608\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0285 - acc: 0.1406 - val_loss: 3.8386 - val_acc: 0.1714\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0503 - acc: 0.1362 - val_loss: 3.8214 - val_acc: 0.1734\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0418 - acc: 0.1392 - val_loss: 3.7628 - val_acc: 0.1834\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9965 - acc: 0.1439 - val_loss: 3.8559 - val_acc: 0.1618\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9794 - acc: 0.1499 - val_loss: 3.7687 - val_acc: 0.1797\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9241 - acc: 0.1547 - val_loss: 3.7737 - val_acc: 0.1777\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9317 - acc: 0.1515 - val_loss: 3.7011 - val_acc: 0.1924\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.9154 - acc: 0.1601 - val_loss: 3.6778 - val_acc: 0.1930\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8986 - acc: 0.1601 - val_loss: 3.6663 - val_acc: 0.1982\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8894 - acc: 0.1593 - val_loss: 3.6324 - val_acc: 0.2008\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8419 - acc: 0.1675 - val_loss: 3.7265 - val_acc: 0.1872\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8310 - acc: 0.1666 - val_loss: 3.6751 - val_acc: 0.1934\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8278 - acc: 0.1714 - val_loss: 3.6259 - val_acc: 0.2039\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8126 - acc: 0.1718 - val_loss: 3.5827 - val_acc: 0.2089\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8274 - acc: 0.1659 - val_loss: 3.5670 - val_acc: 0.2116\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7743 - acc: 0.1741 - val_loss: 3.6353 - val_acc: 0.2006\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7485 - acc: 0.1779 - val_loss: 3.5749 - val_acc: 0.2112\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7568 - acc: 0.1813 - val_loss: 3.5003 - val_acc: 0.2256\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7518 - acc: 0.1813 - val_loss: 3.5315 - val_acc: 0.2183\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7585 - acc: 0.1796 - val_loss: 3.5833 - val_acc: 0.2088\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7112 - acc: 0.1881 - val_loss: 3.5061 - val_acc: 0.2235\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7101 - acc: 0.1867 - val_loss: 3.5634 - val_acc: 0.2150\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7311 - acc: 0.1855 - val_loss: 3.5514 - val_acc: 0.2132\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.7024 - acc: 0.1894 - val_loss: 3.4768 - val_acc: 0.2258\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6952 - acc: 0.1882 - val_loss: 3.4481 - val_acc: 0.2311\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6458 - acc: 0.1964 - val_loss: 3.4979 - val_acc: 0.2245\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6675 - acc: 0.1917 - val_loss: 3.4546 - val_acc: 0.2304\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6629 - acc: 0.1939 - val_loss: 3.4626 - val_acc: 0.2297\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6606 - acc: 0.1956 - val_loss: 3.4788 - val_acc: 0.2248\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.6669 - acc: 0.1949 - val_loss: 3.4797 - val_acc: 0.2227\n",
            "20000/20000 [==============================] - 3s 161us/step\n",
            "Test loss: 3.479693895530701\n",
            "Test accuracy: 0.2227\n",
            "Time taken to compile: 765.8978946208954\n",
            "Training withelu Activation function\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 5.1099 - acc: 0.0226 - val_loss: 4.8800 - val_acc: 0.0468\n",
            "Epoch 2/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.8768 - acc: 0.0444 - val_loss: 4.7135 - val_acc: 0.0593\n",
            "Epoch 3/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.7221 - acc: 0.0606 - val_loss: 4.5765 - val_acc: 0.0775\n",
            "Epoch 4/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.6404 - acc: 0.0693 - val_loss: 4.6455 - val_acc: 0.0776\n",
            "Epoch 5/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.5366 - acc: 0.0829 - val_loss: 4.3277 - val_acc: 0.1083\n",
            "Epoch 6/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.4396 - acc: 0.0892 - val_loss: 4.3149 - val_acc: 0.1134\n",
            "Epoch 7/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.3879 - acc: 0.1003 - val_loss: 4.3706 - val_acc: 0.1036\n",
            "Epoch 8/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.3243 - acc: 0.1063 - val_loss: 4.2560 - val_acc: 0.1210\n",
            "Epoch 9/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2877 - acc: 0.1142 - val_loss: 4.2143 - val_acc: 0.1269\n",
            "Epoch 10/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.2680 - acc: 0.1154 - val_loss: 4.1603 - val_acc: 0.1351\n",
            "Epoch 11/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.1932 - acc: 0.1239 - val_loss: 4.1395 - val_acc: 0.1365\n",
            "Epoch 12/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.1892 - acc: 0.1278 - val_loss: 4.0779 - val_acc: 0.1489\n",
            "Epoch 13/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.1840 - acc: 0.1290 - val_loss: 4.2298 - val_acc: 0.1329\n",
            "Epoch 14/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1675 - acc: 0.1311 - val_loss: 4.2126 - val_acc: 0.1308\n",
            "Epoch 15/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.1531 - acc: 0.1317 - val_loss: 3.9912 - val_acc: 0.1575\n",
            "Epoch 16/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0866 - acc: 0.1388 - val_loss: 4.0975 - val_acc: 0.1421\n",
            "Epoch 17/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0854 - acc: 0.1475 - val_loss: 3.9364 - val_acc: 0.1631\n",
            "Epoch 18/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0934 - acc: 0.1416 - val_loss: 4.0148 - val_acc: 0.1527\n",
            "Epoch 19/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 4.0950 - acc: 0.1432 - val_loss: 3.9593 - val_acc: 0.1651\n",
            "Epoch 20/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0634 - acc: 0.1424 - val_loss: 3.9915 - val_acc: 0.1593\n",
            "Epoch 21/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0285 - acc: 0.1490 - val_loss: 4.0220 - val_acc: 0.1572\n",
            "Epoch 22/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0295 - acc: 0.1477 - val_loss: 4.0012 - val_acc: 0.1588\n",
            "Epoch 23/50\n",
            "500/500 [==============================] - 15s 29ms/step - loss: 4.0411 - acc: 0.1546 - val_loss: 4.0503 - val_acc: 0.1547\n",
            "Epoch 24/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9924 - acc: 0.1579 - val_loss: 3.9580 - val_acc: 0.1664\n",
            "Epoch 25/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 4.0389 - acc: 0.1486 - val_loss: 3.9080 - val_acc: 0.1696\n",
            "Epoch 26/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9689 - acc: 0.1565 - val_loss: 3.8817 - val_acc: 0.1706\n",
            "Epoch 27/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9911 - acc: 0.1586 - val_loss: 4.0140 - val_acc: 0.1559\n",
            "Epoch 28/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9923 - acc: 0.1543 - val_loss: 3.8992 - val_acc: 0.1703\n",
            "Epoch 29/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9817 - acc: 0.1542 - val_loss: 3.8008 - val_acc: 0.1842\n",
            "Epoch 30/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9800 - acc: 0.1584 - val_loss: 3.9533 - val_acc: 0.1696\n",
            "Epoch 31/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9331 - acc: 0.1646 - val_loss: 3.9559 - val_acc: 0.1689\n",
            "Epoch 32/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9500 - acc: 0.1619 - val_loss: 3.8661 - val_acc: 0.1779\n",
            "Epoch 33/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9537 - acc: 0.1628 - val_loss: 4.0169 - val_acc: 0.1646\n",
            "Epoch 34/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9361 - acc: 0.1655 - val_loss: 3.7968 - val_acc: 0.1835\n",
            "Epoch 35/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9597 - acc: 0.1594 - val_loss: 4.0229 - val_acc: 0.1618\n",
            "Epoch 36/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8961 - acc: 0.1697 - val_loss: 3.9098 - val_acc: 0.1805\n",
            "Epoch 37/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9020 - acc: 0.1653 - val_loss: 3.8677 - val_acc: 0.1781\n",
            "Epoch 38/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9418 - acc: 0.1649 - val_loss: 3.9897 - val_acc: 0.1657\n",
            "Epoch 39/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9163 - acc: 0.1654 - val_loss: 3.9897 - val_acc: 0.1686\n",
            "Epoch 40/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9179 - acc: 0.1672 - val_loss: 3.7977 - val_acc: 0.1883\n",
            "Epoch 41/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8822 - acc: 0.1759 - val_loss: 3.8808 - val_acc: 0.1807\n",
            "Epoch 42/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8743 - acc: 0.1722 - val_loss: 3.9122 - val_acc: 0.1734\n",
            "Epoch 43/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9081 - acc: 0.1679 - val_loss: 3.8011 - val_acc: 0.1887\n",
            "Epoch 44/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8869 - acc: 0.1713 - val_loss: 3.9021 - val_acc: 0.1764\n",
            "Epoch 45/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.9120 - acc: 0.1658 - val_loss: 3.7471 - val_acc: 0.1965\n",
            "Epoch 46/50\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.8544 - acc: 0.1745 - val_loss: 3.7260 - val_acc: 0.1950\n",
            "Epoch 47/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8565 - acc: 0.1779 - val_loss: 3.9822 - val_acc: 0.1706\n",
            "Epoch 48/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8622 - acc: 0.1764 - val_loss: 3.7978 - val_acc: 0.1918\n",
            "Epoch 49/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8939 - acc: 0.1729 - val_loss: 3.7846 - val_acc: 0.1905\n",
            "Epoch 50/50\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.8640 - acc: 0.1753 - val_loss: 3.7810 - val_acc: 0.1914\n",
            "20000/20000 [==============================] - 3s 164us/step\n",
            "Test loss: 3.780985068321228\n",
            "Test accuracy: 0.1914\n",
            "Time taken to compile: 759.1990141868591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "duC1UM67xqZQ",
        "colab_type": "code",
        "outputId": "b54d1ba7-3864-40bd-8251-4d53aac56f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "Activation_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['tanh', 0.1809], ['relu', 0.2227], ['elu', 0.1914]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "am_bFTsqx1QR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy was high when relu activation function is used\n",
        "**Next we shall change the number of epochs keeping Adamax optimizer and relu activation**"
      ]
    },
    {
      "metadata": {
        "id": "iW9eHIDryOlM",
        "colab_type": "code",
        "outputId": "2dc0cfe9-a6a3-4fbc-e083-38084df9f6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7889
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = [75,100]\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "Epoch_accuracy=[]\n",
        "batch_size = 32\n",
        "data_augmentation = True\n",
        "for ep in epochs:\n",
        "  epochs = ep\n",
        "  print('Training with ' + str(ep) + ' epochs')\n",
        "  logs = \"logs/optimizer/\"+str(ep)\n",
        "    \n",
        "  model_name = str(ep) + 'epochs for TinyImagenet200'\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # initiate RMSprop optimizer\n",
        "\n",
        "  model.summary()\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adamax(),metrics=['accuracy'])\n",
        "\n",
        "  start = time()\n",
        "  \n",
        "  if not data_augmentation:\n",
        "      print('Not using data augmentation.')\n",
        "      model.fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  shuffle=True)\n",
        "  else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            steps_per_epoch=500)\n",
        "\n",
        "    # Save model and weights\n",
        "  if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "  print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "    # Score trained model.\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "  end = time()\n",
        "\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  print('Time taken to compile:', str(end-start))\n",
        "  Epoch_accuracy.append([ep,scores[1]])\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with 75 epochs\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/75\n",
            "500/500 [==============================] - 22s 43ms/step - loss: 5.2658 - acc: 0.0073 - val_loss: 5.1935 - val_acc: 0.0135\n",
            "Epoch 2/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 5.1602 - acc: 0.0139 - val_loss: 5.0645 - val_acc: 0.0190\n",
            "Epoch 3/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 5.0885 - acc: 0.0181 - val_loss: 4.9982 - val_acc: 0.0262\n",
            "Epoch 4/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 5.0168 - acc: 0.0241 - val_loss: 4.9063 - val_acc: 0.0378\n",
            "Epoch 5/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.9304 - acc: 0.0323 - val_loss: 4.7881 - val_acc: 0.0522\n",
            "Epoch 6/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.8112 - acc: 0.0447 - val_loss: 4.6537 - val_acc: 0.0609\n",
            "Epoch 7/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.7397 - acc: 0.0509 - val_loss: 4.5996 - val_acc: 0.0669\n",
            "Epoch 8/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.6559 - acc: 0.0581 - val_loss: 4.5249 - val_acc: 0.0761\n",
            "Epoch 9/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.6030 - acc: 0.0644 - val_loss: 4.4537 - val_acc: 0.0887\n",
            "Epoch 10/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.5426 - acc: 0.0731 - val_loss: 4.3837 - val_acc: 0.0968\n",
            "Epoch 11/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.4665 - acc: 0.0781 - val_loss: 4.2678 - val_acc: 0.1080\n",
            "Epoch 12/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.4201 - acc: 0.0872 - val_loss: 4.2640 - val_acc: 0.1154\n",
            "Epoch 13/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.3792 - acc: 0.0922 - val_loss: 4.1989 - val_acc: 0.1182\n",
            "Epoch 14/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.3263 - acc: 0.0973 - val_loss: 4.1599 - val_acc: 0.1246\n",
            "Epoch 15/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.3122 - acc: 0.1006 - val_loss: 4.0709 - val_acc: 0.1375\n",
            "Epoch 16/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.2373 - acc: 0.1135 - val_loss: 4.1031 - val_acc: 0.1348\n",
            "Epoch 17/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.2305 - acc: 0.1165 - val_loss: 4.0436 - val_acc: 0.1443\n",
            "Epoch 18/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.2026 - acc: 0.1159 - val_loss: 3.9655 - val_acc: 0.1535\n",
            "Epoch 19/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.1740 - acc: 0.1199 - val_loss: 3.9552 - val_acc: 0.1535\n",
            "Epoch 20/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.1455 - acc: 0.1249 - val_loss: 3.9482 - val_acc: 0.1599\n",
            "Epoch 21/75\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0871 - acc: 0.1328 - val_loss: 3.8910 - val_acc: 0.1603\n",
            "Epoch 22/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.0825 - acc: 0.1311 - val_loss: 3.9256 - val_acc: 0.1588\n",
            "Epoch 23/75\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 4.0610 - acc: 0.1349 - val_loss: 3.9203 - val_acc: 0.1583\n",
            "Epoch 24/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.0393 - acc: 0.1411 - val_loss: 3.8650 - val_acc: 0.1681\n",
            "Epoch 25/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.0315 - acc: 0.1409 - val_loss: 3.8200 - val_acc: 0.1780\n",
            "Epoch 26/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9902 - acc: 0.1449 - val_loss: 3.7659 - val_acc: 0.1824\n",
            "Epoch 27/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9505 - acc: 0.1551 - val_loss: 3.7672 - val_acc: 0.1832\n",
            "Epoch 28/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9359 - acc: 0.1550 - val_loss: 3.8094 - val_acc: 0.1751\n",
            "Epoch 29/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9455 - acc: 0.1543 - val_loss: 3.7367 - val_acc: 0.1850\n",
            "Epoch 30/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.9473 - acc: 0.1524 - val_loss: 3.7235 - val_acc: 0.1845\n",
            "Epoch 31/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8737 - acc: 0.1639 - val_loss: 3.6581 - val_acc: 0.2014\n",
            "Epoch 32/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8753 - acc: 0.1618 - val_loss: 3.6571 - val_acc: 0.1981\n",
            "Epoch 33/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.8497 - acc: 0.1629 - val_loss: 3.6435 - val_acc: 0.2006\n",
            "Epoch 34/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8475 - acc: 0.1669 - val_loss: 3.6146 - val_acc: 0.2067\n",
            "Epoch 35/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8320 - acc: 0.1668 - val_loss: 3.6177 - val_acc: 0.2051\n",
            "Epoch 36/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8113 - acc: 0.1704 - val_loss: 3.6063 - val_acc: 0.2093\n",
            "Epoch 37/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.8073 - acc: 0.1717 - val_loss: 3.5677 - val_acc: 0.2097\n",
            "Epoch 38/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7550 - acc: 0.1816 - val_loss: 3.5801 - val_acc: 0.2111\n",
            "Epoch 39/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7667 - acc: 0.1797 - val_loss: 3.5545 - val_acc: 0.2144\n",
            "Epoch 40/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7498 - acc: 0.1806 - val_loss: 3.6233 - val_acc: 0.2032\n",
            "Epoch 41/75\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.7200 - acc: 0.1849 - val_loss: 3.5395 - val_acc: 0.2154\n",
            "Epoch 42/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7340 - acc: 0.1819 - val_loss: 3.5294 - val_acc: 0.2142\n",
            "Epoch 43/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7314 - acc: 0.1845 - val_loss: 3.5102 - val_acc: 0.2204\n",
            "Epoch 44/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.7134 - acc: 0.1806 - val_loss: 3.5156 - val_acc: 0.2185\n",
            "Epoch 45/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.7053 - acc: 0.1914 - val_loss: 3.4798 - val_acc: 0.2293\n",
            "Epoch 46/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6575 - acc: 0.1896 - val_loss: 3.5224 - val_acc: 0.2195\n",
            "Epoch 47/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.6649 - acc: 0.1907 - val_loss: 3.4727 - val_acc: 0.2246\n",
            "Epoch 48/75\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.6702 - acc: 0.1953 - val_loss: 3.5415 - val_acc: 0.2142\n",
            "Epoch 49/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6711 - acc: 0.1934 - val_loss: 3.4780 - val_acc: 0.2273\n",
            "Epoch 50/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6520 - acc: 0.1986 - val_loss: 3.4406 - val_acc: 0.2364\n",
            "Epoch 51/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6142 - acc: 0.2023 - val_loss: 3.4285 - val_acc: 0.2376\n",
            "Epoch 52/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6151 - acc: 0.1991 - val_loss: 3.4299 - val_acc: 0.2334\n",
            "Epoch 53/75\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.5988 - acc: 0.2083 - val_loss: 3.4371 - val_acc: 0.2340\n",
            "Epoch 54/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.6269 - acc: 0.2035 - val_loss: 3.4377 - val_acc: 0.2320\n",
            "Epoch 55/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6202 - acc: 0.2009 - val_loss: 3.4032 - val_acc: 0.2371\n",
            "Epoch 56/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5549 - acc: 0.2073 - val_loss: 3.4385 - val_acc: 0.2301\n",
            "Epoch 57/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5785 - acc: 0.2121 - val_loss: 3.3741 - val_acc: 0.2445\n",
            "Epoch 58/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5885 - acc: 0.2049 - val_loss: 3.4256 - val_acc: 0.2352\n",
            "Epoch 59/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5716 - acc: 0.2051 - val_loss: 3.4410 - val_acc: 0.2294\n",
            "Epoch 60/75\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 3.5783 - acc: 0.2090 - val_loss: 3.3697 - val_acc: 0.2427\n",
            "Epoch 61/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5230 - acc: 0.2157 - val_loss: 3.3875 - val_acc: 0.2382\n",
            "Epoch 62/75\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.5312 - acc: 0.2167 - val_loss: 3.4008 - val_acc: 0.2424\n",
            "Epoch 63/75\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.5358 - acc: 0.2136 - val_loss: 3.4298 - val_acc: 0.2351\n",
            "Epoch 64/75\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5565 - acc: 0.2134 - val_loss: 3.3962 - val_acc: 0.2384\n",
            "Epoch 65/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5523 - acc: 0.2110 - val_loss: 3.4463 - val_acc: 0.2301\n",
            "Epoch 66/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5138 - acc: 0.2176 - val_loss: 3.3756 - val_acc: 0.2439\n",
            "Epoch 67/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4991 - acc: 0.2176 - val_loss: 3.3988 - val_acc: 0.2357\n",
            "Epoch 68/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5205 - acc: 0.2173 - val_loss: 3.3289 - val_acc: 0.2536\n",
            "Epoch 69/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5138 - acc: 0.2181 - val_loss: 3.3324 - val_acc: 0.2508\n",
            "Epoch 70/75\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.5177 - acc: 0.2134 - val_loss: 3.3374 - val_acc: 0.2529\n",
            "Epoch 71/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4720 - acc: 0.2268 - val_loss: 3.3824 - val_acc: 0.2434\n",
            "Epoch 72/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4763 - acc: 0.2259 - val_loss: 3.3265 - val_acc: 0.2530\n",
            "Epoch 73/75\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4976 - acc: 0.2171 - val_loss: 3.3359 - val_acc: 0.2505\n",
            "Epoch 74/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4937 - acc: 0.2221 - val_loss: 3.4048 - val_acc: 0.2404\n",
            "Epoch 75/75\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4925 - acc: 0.2251 - val_loss: 3.2971 - val_acc: 0.2596\n",
            "Saved trained model at /content/saved_models/75epochs for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 148us/step\n",
            "Test loss: 3.2971280855178833\n",
            "Test accuracy: 0.2596\n",
            "Time taken to compile: 1233.9288890361786\n",
            "Training with 100 epochs\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 200)               0         \n",
            "=================================================================\n",
            "Total params: 1,348,328\n",
            "Trainable params: 1,348,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.2637 - acc: 0.0079 - val_loss: 5.1711 - val_acc: 0.0110\n",
            "Epoch 2/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 5.1432 - acc: 0.0132 - val_loss: 5.0771 - val_acc: 0.0165\n",
            "Epoch 3/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 5.0872 - acc: 0.0187 - val_loss: 4.9917 - val_acc: 0.0267\n",
            "Epoch 4/100\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 5.0169 - acc: 0.0251 - val_loss: 4.8771 - val_acc: 0.0389\n",
            "Epoch 5/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.9117 - acc: 0.0345 - val_loss: 4.7171 - val_acc: 0.0525\n",
            "Epoch 6/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.7779 - acc: 0.0455 - val_loss: 4.5874 - val_acc: 0.0688\n",
            "Epoch 7/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.7051 - acc: 0.0556 - val_loss: 4.4965 - val_acc: 0.0827\n",
            "Epoch 8/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.6372 - acc: 0.0592 - val_loss: 4.4396 - val_acc: 0.0885\n",
            "Epoch 9/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.5704 - acc: 0.0700 - val_loss: 4.3625 - val_acc: 0.1024\n",
            "Epoch 10/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.4960 - acc: 0.0764 - val_loss: 4.2694 - val_acc: 0.1124\n",
            "Epoch 11/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.4278 - acc: 0.0824 - val_loss: 4.2194 - val_acc: 0.1182\n",
            "Epoch 12/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.3785 - acc: 0.0899 - val_loss: 4.2903 - val_acc: 0.1016\n",
            "Epoch 13/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.3534 - acc: 0.0970 - val_loss: 4.1434 - val_acc: 0.1270\n",
            "Epoch 14/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.2783 - acc: 0.1055 - val_loss: 4.0793 - val_acc: 0.1338\n",
            "Epoch 15/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.2735 - acc: 0.1076 - val_loss: 4.1014 - val_acc: 0.1312\n",
            "Epoch 16/100\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 4.2072 - acc: 0.1144 - val_loss: 3.9898 - val_acc: 0.1445\n",
            "Epoch 17/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 4.1612 - acc: 0.1239 - val_loss: 3.9718 - val_acc: 0.1517\n",
            "Epoch 18/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.1326 - acc: 0.1229 - val_loss: 3.8898 - val_acc: 0.1661\n",
            "Epoch 19/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.1057 - acc: 0.1304 - val_loss: 3.8814 - val_acc: 0.1638\n",
            "Epoch 20/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.0582 - acc: 0.1362 - val_loss: 3.8811 - val_acc: 0.1628\n",
            "Epoch 21/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 4.0090 - acc: 0.1444 - val_loss: 3.9185 - val_acc: 0.1594\n",
            "Epoch 22/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 4.0022 - acc: 0.1425 - val_loss: 3.7546 - val_acc: 0.1865\n",
            "Epoch 23/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9721 - acc: 0.1483 - val_loss: 3.7504 - val_acc: 0.1848\n",
            "Epoch 24/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.9652 - acc: 0.1460 - val_loss: 3.7694 - val_acc: 0.1851\n",
            "Epoch 25/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.9415 - acc: 0.1499 - val_loss: 3.7377 - val_acc: 0.1836\n",
            "Epoch 26/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.8738 - acc: 0.1576 - val_loss: 3.7185 - val_acc: 0.1880\n",
            "Epoch 27/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.8703 - acc: 0.1611 - val_loss: 3.7511 - val_acc: 0.1881\n",
            "Epoch 28/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.8883 - acc: 0.1591 - val_loss: 3.6136 - val_acc: 0.2059\n",
            "Epoch 29/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.8558 - acc: 0.1666 - val_loss: 3.6064 - val_acc: 0.2092\n",
            "Epoch 30/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.8343 - acc: 0.1658 - val_loss: 3.6115 - val_acc: 0.2061\n",
            "Epoch 31/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7732 - acc: 0.1768 - val_loss: 3.6021 - val_acc: 0.2103\n",
            "Epoch 32/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7903 - acc: 0.1723 - val_loss: 3.6155 - val_acc: 0.2090\n",
            "Epoch 33/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7743 - acc: 0.1709 - val_loss: 3.5934 - val_acc: 0.2097\n",
            "Epoch 34/100\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.7851 - acc: 0.1728 - val_loss: 3.5719 - val_acc: 0.2141\n",
            "Epoch 35/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7862 - acc: 0.1806 - val_loss: 3.6352 - val_acc: 0.2002\n",
            "Epoch 36/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.7108 - acc: 0.1846 - val_loss: 3.5335 - val_acc: 0.2183\n",
            "Epoch 37/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.7360 - acc: 0.1829 - val_loss: 3.4771 - val_acc: 0.2283\n",
            "Epoch 38/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.7203 - acc: 0.1864 - val_loss: 3.5563 - val_acc: 0.2147\n",
            "Epoch 39/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.7113 - acc: 0.1867 - val_loss: 3.5619 - val_acc: 0.2167\n",
            "Epoch 40/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6996 - acc: 0.1867 - val_loss: 3.5196 - val_acc: 0.2231\n",
            "Epoch 41/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6702 - acc: 0.1956 - val_loss: 3.4951 - val_acc: 0.2274\n",
            "Epoch 42/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6635 - acc: 0.1973 - val_loss: 3.4448 - val_acc: 0.2338\n",
            "Epoch 43/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6561 - acc: 0.1964 - val_loss: 3.5451 - val_acc: 0.2186\n",
            "Epoch 44/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.6524 - acc: 0.1956 - val_loss: 3.4592 - val_acc: 0.2292\n",
            "Epoch 45/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.6718 - acc: 0.1900 - val_loss: 3.4993 - val_acc: 0.2247\n",
            "Epoch 46/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6015 - acc: 0.2032 - val_loss: 3.4933 - val_acc: 0.2278\n",
            "Epoch 47/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.6227 - acc: 0.2022 - val_loss: 3.4480 - val_acc: 0.2308\n",
            "Epoch 48/100\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 3.6136 - acc: 0.2024 - val_loss: 3.4807 - val_acc: 0.2283\n",
            "Epoch 49/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.6070 - acc: 0.2038 - val_loss: 3.4458 - val_acc: 0.2343\n",
            "Epoch 50/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.6180 - acc: 0.2025 - val_loss: 3.4741 - val_acc: 0.2246\n",
            "Epoch 51/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5898 - acc: 0.2054 - val_loss: 3.4327 - val_acc: 0.2326\n",
            "Epoch 52/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5699 - acc: 0.2071 - val_loss: 3.3979 - val_acc: 0.2415\n",
            "Epoch 53/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5669 - acc: 0.2080 - val_loss: 3.3898 - val_acc: 0.2417\n",
            "Epoch 54/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5974 - acc: 0.2009 - val_loss: 3.3832 - val_acc: 0.2445\n",
            "Epoch 55/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5875 - acc: 0.2030 - val_loss: 3.3563 - val_acc: 0.2469\n",
            "Epoch 56/100\n",
            "500/500 [==============================] - 16s 31ms/step - loss: 3.5156 - acc: 0.2203 - val_loss: 3.4513 - val_acc: 0.2364\n",
            "Epoch 57/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.5341 - acc: 0.2150 - val_loss: 3.3628 - val_acc: 0.2482\n",
            "Epoch 58/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5237 - acc: 0.2195 - val_loss: 3.4171 - val_acc: 0.2379\n",
            "Epoch 59/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5510 - acc: 0.2133 - val_loss: 3.4179 - val_acc: 0.2386\n",
            "Epoch 60/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5445 - acc: 0.2113 - val_loss: 3.4024 - val_acc: 0.2417\n",
            "Epoch 61/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4980 - acc: 0.2201 - val_loss: 3.3579 - val_acc: 0.2473\n",
            "Epoch 62/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.5028 - acc: 0.2202 - val_loss: 3.3698 - val_acc: 0.2419\n",
            "Epoch 63/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.5012 - acc: 0.2179 - val_loss: 3.3854 - val_acc: 0.2428\n",
            "Epoch 64/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5043 - acc: 0.2189 - val_loss: 3.3771 - val_acc: 0.2418\n",
            "Epoch 65/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.5282 - acc: 0.2149 - val_loss: 3.3944 - val_acc: 0.2418\n",
            "Epoch 66/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4694 - acc: 0.2225 - val_loss: 3.3388 - val_acc: 0.2493\n",
            "Epoch 67/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4835 - acc: 0.2239 - val_loss: 3.4066 - val_acc: 0.2379\n",
            "Epoch 68/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4946 - acc: 0.2222 - val_loss: 3.3243 - val_acc: 0.2541\n",
            "Epoch 69/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4693 - acc: 0.2223 - val_loss: 3.3166 - val_acc: 0.2564\n",
            "Epoch 70/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4538 - acc: 0.2270 - val_loss: 3.3358 - val_acc: 0.2542\n",
            "Epoch 71/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4383 - acc: 0.2292 - val_loss: 3.3528 - val_acc: 0.2488\n",
            "Epoch 72/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4681 - acc: 0.2241 - val_loss: 3.3164 - val_acc: 0.2561\n",
            "Epoch 73/100\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 3.4509 - acc: 0.2264 - val_loss: 3.3323 - val_acc: 0.2530\n",
            "Epoch 74/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4905 - acc: 0.2212 - val_loss: 3.2835 - val_acc: 0.2610\n",
            "Epoch 75/100\n",
            "500/500 [==============================] - 16s 33ms/step - loss: 3.4750 - acc: 0.2194 - val_loss: 3.3458 - val_acc: 0.2505\n",
            "Epoch 76/100\n",
            "500/500 [==============================] - 16s 32ms/step - loss: 3.4340 - acc: 0.2302 - val_loss: 3.3447 - val_acc: 0.2527\n",
            "Epoch 77/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.4256 - acc: 0.2316 - val_loss: 3.3082 - val_acc: 0.2550\n",
            "Epoch 78/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.4340 - acc: 0.2306 - val_loss: 3.3340 - val_acc: 0.2498\n",
            "Epoch 79/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.4274 - acc: 0.2267 - val_loss: 3.3663 - val_acc: 0.2483\n",
            "Epoch 80/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.4439 - acc: 0.2263 - val_loss: 3.2935 - val_acc: 0.2581\n",
            "Epoch 81/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4274 - acc: 0.2322 - val_loss: 3.3486 - val_acc: 0.2457\n",
            "Epoch 82/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4024 - acc: 0.2360 - val_loss: 3.3058 - val_acc: 0.2561\n",
            "Epoch 83/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4035 - acc: 0.2341 - val_loss: 3.3079 - val_acc: 0.2517\n",
            "Epoch 84/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4059 - acc: 0.2369 - val_loss: 3.2077 - val_acc: 0.2751\n",
            "Epoch 85/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4430 - acc: 0.2239 - val_loss: 3.2709 - val_acc: 0.2656\n",
            "Epoch 86/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3913 - acc: 0.2342 - val_loss: 3.2633 - val_acc: 0.2647\n",
            "Epoch 87/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3692 - acc: 0.2416 - val_loss: 3.2899 - val_acc: 0.2604\n",
            "Epoch 88/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3922 - acc: 0.2345 - val_loss: 3.3503 - val_acc: 0.2509\n",
            "Epoch 89/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4112 - acc: 0.2329 - val_loss: 3.3744 - val_acc: 0.2466\n",
            "Epoch 90/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.4064 - acc: 0.2379 - val_loss: 3.2395 - val_acc: 0.2664\n",
            "Epoch 91/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3580 - acc: 0.2438 - val_loss: 3.2670 - val_acc: 0.2651\n",
            "Epoch 92/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.3861 - acc: 0.2334 - val_loss: 3.3384 - val_acc: 0.2530\n",
            "Epoch 93/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3711 - acc: 0.2382 - val_loss: 3.2474 - val_acc: 0.2636\n",
            "Epoch 94/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3711 - acc: 0.2391 - val_loss: 3.2718 - val_acc: 0.2622\n",
            "Epoch 95/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3800 - acc: 0.2397 - val_loss: 3.3347 - val_acc: 0.2500\n",
            "Epoch 96/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.3358 - acc: 0.2477 - val_loss: 3.2008 - val_acc: 0.2775\n",
            "Epoch 97/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3497 - acc: 0.2441 - val_loss: 3.2705 - val_acc: 0.2647\n",
            "Epoch 98/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.3678 - acc: 0.2408 - val_loss: 3.3059 - val_acc: 0.2596\n",
            "Epoch 99/100\n",
            "500/500 [==============================] - 15s 31ms/step - loss: 3.3794 - acc: 0.2401 - val_loss: 3.2709 - val_acc: 0.2635\n",
            "Epoch 100/100\n",
            "500/500 [==============================] - 15s 30ms/step - loss: 3.3668 - acc: 0.2398 - val_loss: 3.2219 - val_acc: 0.2711\n",
            "Saved trained model at /content/saved_models/100epochs for TinyImagenet200 \n",
            "20000/20000 [==============================] - 3s 154us/step\n",
            "Test loss: 3.2218521209716795\n",
            "Test accuracy: 0.2711\n",
            "Time taken to compile: 1616.324007511139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OKqN3yZoYJv9",
        "colab_type": "code",
        "outputId": "172492be-b132-4844-8e32-7398eec32963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "Epoch_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[75, 0.2596], [100, 0.2711]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "IERu0-2rI9er",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}